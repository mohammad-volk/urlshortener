import requests
from bs4 import BeautifulSoup
from django.core.mail import send_mail
from django.conf import settings
import geoip2.database
import geoip2.errors
from reportlab.pdfgen import canvas
from reportlab.lib.pagesizes import letter
import csv
from django.http import HttpResponse
from django.utils import timezone
from django.db.models import Count
from datetime import timedelta

def extract_url_info(url):
    """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„ØµÙØ­Ø© Ù…Ù† Ø§Ù„Ø±Ø§Ø¨Ø·"""
    try:
        response = requests.get(url, timeout=10, headers={
            'User-Agent': 'Mozilla/5.0 (compatible; UrlPro/1.0)'
        })
        soup = BeautifulSoup(response.content, 'html.parser')
        
        title = soup.find('title')
        title = title.get_text().strip() if title else ''
        
        description = soup.find('meta', attrs={'name': 'description'})
        description = description.get('content', '').strip() if description else ''
        
        return {
            'title': title[:200],
            'description': description[:300],
            'status_code': response.status_code
        }
    except:
        return {'title': '', 'description': '', 'status_code': 0}

def get_location_from_ip(ip_address):
    """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙˆÙ‚Ø¹ Ø§Ù„Ø¬ØºØ±Ø§ÙÙŠ Ù…Ù† IP"""
    try:
        # ÙŠÙ…ÙƒÙ†Ùƒ ØªØ­Ù…ÙŠÙ„ Ù‚Ø§Ø¹Ø¯Ø© Ø¨ÙŠØ§Ù†Ø§Øª GeoLite2 Ù…Ø¬Ø§Ù†Ø§Ù‹ Ù…Ù† MaxMind
        with geoip2.database.Reader('path/to/GeoLite2-City.mmdb') as reader:
            response = reader.city(ip_address)
            return {
                'country': response.country.names.get('ar', response.country.name),
                'city': response.city.names.get('ar', response.city.name),
            }
    except:
        return {'country': 'ØºÙŠØ± Ù…Ø¹Ø±ÙˆÙ', 'city': 'ØºÙŠØ± Ù…Ø¹Ø±ÙˆÙ'}

def generate_csv_export(user):
    """ØªØµØ¯ÙŠØ± Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø¥Ù„Ù‰ CSV"""
    response = HttpResponse(content_type='text/csv')
    response['Content-Disposition'] = 'attachment; filename="urls_export.csv"'
    
    writer = csv.writer(response)
    writer.writerow(['Ø§Ù„Ø±Ø§Ø¨Ø· Ø§Ù„Ø£ØµÙ„ÙŠ', 'Ø§Ù„Ø±Ø§Ø¨Ø· Ø§Ù„Ù…Ø®ØªØµØ±', 'Ø§Ù„Ø¹Ù†ÙˆØ§Ù†', 'Ø§Ù„Ù†Ù‚Ø±Ø§Øª', 'ØªØ§Ø±ÙŠØ® Ø§Ù„Ø¥Ù†Ø´Ø§Ø¡'])
    
    for url in user.url_set.all():
        writer.writerow([
            url.original_url,
            url.get_short_url(),
            url.title,
            url.click_count,
            url.created_at.strftime('%Y-%m-%d %H:%M')
        ])
    
    return response

def generate_pdf_report(user):
    """Ø¥Ù†Ø´Ø§Ø¡ ØªÙ‚Ø±ÙŠØ± PDF Ù„Ù„Ù…Ø³ØªØ®Ø¯Ù…"""
    response = HttpResponse(content_type='application/pdf')
    response['Content-Disposition'] = 'attachment; filename="monthly_report.pdf"'
    
    p = canvas.Canvas(response, pagesize=letter)
    
    # Ø¹Ù†ÙˆØ§Ù† Ø§Ù„ØªÙ‚Ø±ÙŠØ±
    p.drawString(100, 750, f"ØªÙ‚Ø±ÙŠØ± Ø´Ù‡Ø±ÙŠ Ù„Ù„Ù…Ø³ØªØ®Ø¯Ù…: {user.username}")
    
    # Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
    y_position = 700
    urls = user.url_set.all()
    total_urls = urls.count()
    total_clicks = sum(urls.values_list('click_count', flat=True))
    
    p.drawString(100, y_position, f"Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ø±ÙˆØ§Ø¨Ø·: {total_urls}")
    p.drawString(100, y_position-20, f"Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ù†Ù‚Ø±Ø§Øª: {total_clicks}")
    
    # Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ø±ÙˆØ§Ø¨Ø·
    y_position -= 60
    p.drawString(100, y_position, "Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ø±ÙˆØ§Ø¨Ø·:")
    y_position -= 20
    
    for url in urls[:20]:  # Ø£ÙˆÙ„ 20 Ø±Ø§Ø¨Ø·
        p.drawString(120, y_position, f"â€¢ {url.title or url.original_url[:50]}...")
        p.drawString(400, y_position, f"{url.click_count} Ù†Ù‚Ø±Ø©")
        y_position -= 15
        
        if y_position < 100:
            break
    
    p.showPage()
    p.save()
    return response

def send_weekly_report(user):
    """Ø¥Ø±Ø³Ø§Ù„ ØªÙ‚Ø±ÙŠØ± Ø£Ø³Ø¨ÙˆØ¹ÙŠ Ø¨Ø§Ù„Ø¨Ø±ÙŠØ¯ Ø§Ù„Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠ"""
    if not user.userprofile.weekly_reports:
        return
    
    urls = user.url_set.all()
    total_clicks_week = sum(
        url.analytics.filter(
            clicked_at__gte=timezone.now() - timedelta(days=7)
        ).count() for url in urls
    )
    
    subject = f'ØªÙ‚Ø±ÙŠØ±Ùƒ Ø§Ù„Ø£Ø³Ø¨ÙˆØ¹ÙŠ - UrlPro'
    message = f'''
    Ù…Ø±Ø­Ø¨Ø§Ù‹ {user.username}ØŒ
    
    Ø¥Ù„ÙŠÙƒ ØªÙ‚Ø±ÙŠØ±Ùƒ Ø§Ù„Ø£Ø³Ø¨ÙˆØ¹ÙŠ:
    
    ğŸ“Š Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ù†Ù‚Ø±Ø§Øª Ù‡Ø°Ø§ Ø§Ù„Ø£Ø³Ø¨ÙˆØ¹: {total_clicks_week}
    ğŸ”— Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ù†Ø´Ø·Ø©: {urls.filter(is_active=True).count()}
    
    ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø±Ø§Ø¬Ø¹Ø© Ø§Ù„ØªÙØ§ØµÙŠÙ„ Ø§Ù„ÙƒØ§Ù…Ù„Ø© ÙÙŠ Ù„ÙˆØ­Ø© Ø§Ù„ØªØ­ÙƒÙ….
    
    ØªØ­ÙŠØ§ØªÙ†Ø§ØŒ
    ÙØ±ÙŠÙ‚ UrlPro
    '''
    
    send_mail(
        subject,
        message,
        settings.DEFAULT_FROM_EMAIL,
        [user.email],
        fail_silently=True,
    )

class RateLimiter:
    """Ù†Ø¸Ø§Ù… ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø¹Ø¯Ù„ Ù„Ù„API"""
    
    @staticmethod
    def check_rate_limit(user_profile, limit_type='api'):
        if limit_type == 'api':
            if user_profile.api_calls_count >= user_profile.api_calls_limit:
                return False
            user_profile.api_calls_count += 1
            user_profile.save()
        return True

def clean_expired_urls():
    """ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ù…Ù†ØªÙ‡ÙŠØ© Ø§Ù„ØµÙ„Ø§Ø­ÙŠØ©"""
    from .models import URL
    expired_urls = URL.objects.filter(
        expires_at__lt=timezone.now(),
        is_active=True
    )
    
    count = expired_urls.count()
    expired_urls.update(is_active=False)
    
    return count

def generate_analytics_data(url_obj, days=30):
    """Ø¥Ù†Ø´Ø§Ø¡ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ­Ù„ÙŠÙ„Ø§Øª Ù„Ø±Ø§Ø¨Ø· Ù…Ø¹ÙŠÙ†"""
    from datetime import datetime, timedelta
    from .models import ClickAnalytics
    
    end_date = timezone.now()
    start_date = end_date - timedelta(days=days)
    
    analytics = url_obj.analytics.filter(
        clicked_at__gte=start_date,
        clicked_at__lte=end_date
    )
    
    # ØªØ¬Ù…ÙŠØ¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø­Ø³Ø¨ Ø§Ù„ÙŠÙˆÙ…
    daily_clicks = {}
    for i in range(days):
        date = start_date + timedelta(days=i)
        daily_clicks[date.strftime('%Y-%m-%d')] = 0
    
    for click in analytics:
        date_str = click.clicked_at.strftime('%Y-%m-%d')
        if date_str in daily_clicks:
            daily_clicks[date_str] += 1
    
    return {
        'daily_clicks': daily_clicks,
        'total_clicks': analytics.count(),
        'unique_clicks': analytics.filter(is_unique=True).count(),
        'countries': list(analytics.values('country').annotate(
            count=Count('id')
        ).order_by('-count')[:10]),
        'devices': list(analytics.values('device_type').annotate(
            count=Count('id')
        ).order_by('-count')),
        'browsers': list(analytics.values('browser').annotate(
            count=Count('id')
        ).order_by('-count')[:10]),
    }